{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source and destination names\n",
    "TICKER = \"AAPL\"\n",
    "YEAR = 2023\n",
    "MASK_NAME = \"ORENIJI\"\n",
    "DATASET_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:54:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Identity of the Edgar REST client set to <span style=\"font-weight: bold\">[</span>gary gary@financialdatasets.org<span style=\"font-weight: bold\">]</span>          <a href=\"file:///Users/dfu/Documents/Git/financial-datasets/venv/lib/python3.12/site-packages/edgar/core.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">core.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/dfu/Documents/Git/financial-datasets/venv/lib/python3.12/site-packages/edgar/core.py#155\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:54:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Identity of the Edgar REST client set to \u001b[1m[\u001b[0mgary gary@financialdatasets.org\u001b[1m]\u001b[0m          \u001b]8;id=38150;file:///Users/dfu/Documents/Git/financial-datasets/venv/lib/python3.12/site-packages/edgar/core.py\u001b\\\u001b[2mcore.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=935189;file:///Users/dfu/Documents/Git/financial-datasets/venv/lib/python3.12/site-packages/edgar/core.py#155\u001b\\\u001b[2m155\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|\u001b[32m██████████\u001b[0m| 10/10 [00:31<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# This cell will use financial_datasets to generate a dataset from a 10-K report\n",
    "# for the given TICKER company and year.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from financial_datasets.generator import DatasetGenerator\n",
    "\n",
    "# Create dataset generator\n",
    "generator = DatasetGenerator(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Generate dataset from 10-K\n",
    "dataset = generator.generate_from_10K(\n",
    "    ticker=TICKER,\n",
    "    year=YEAR,\n",
    "    max_questions=DATASET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for getting the next file name in directory\n",
    "def get_latest_file(JSON_DIR, FILE_PREFIX):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(JSON_DIR) if f.startswith(FILE_PREFIX) and f.endswith('.json')]\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(JSON_DIR, exist_ok=True)\n",
    "        files = []\n",
    "    print(files)\n",
    "    return max(files) if files else FILE_PREFIX + '00.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(\"datasets\", TICKER, \"raw\"), exist_ok=True)\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for id,line in enumerate(dataset.items):\n",
    "    item = {}\n",
    "    item['question'] = line.question\n",
    "    item['answer'] = line.answer\n",
    "    item['context'] = line.context\n",
    "    item['id'] = id\n",
    "    data.append(item)\n",
    "\n",
    "# For keeping our directory organized\n",
    "FILE_PREFIX = \"dataset_\"\n",
    "JSON_DIR = os.path.join(\"datasets\", TICKER, \"raw\")\n",
    "latest_file = get_latest_file(JSON_DIR, FILE_PREFIX)\n",
    "file_num = int(latest_file[-6:-5])\n",
    "next_file_num = file_num + 1\n",
    "next_output_file = f\"{FILE_PREFIX}{next_file_num:02d}.json\"\n",
    "next_output_path = os.path.join(JSON_DIR, next_output_file)\n",
    "# Save the data to JSON\n",
    "with open (next_output_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all separately generated files into one\n",
    "all_data = []\n",
    "current_id = 0\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            for idx in range(len(data)):\n",
    "                data[idx]['id'] = current_id\n",
    "                current_id += 1\n",
    "            all_data.extend(data)\n",
    "\n",
    "all_data_path = os.path.join(\"datasets\", TICKER, TICKER+\".json\")\n",
    "with open(all_data_path, \"w\") as f:\n",
    "    json.dump(all_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the JSON file to JSONL format for fine-tuning\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def convert_to_jsonl(input_data, output_file):\n",
    "    \"\"\"\n",
    "    Convert JSON array to JSONL format and save to file.\n",
    "    \n",
    "    Args:\n",
    "        input_data (list): List of dictionaries containing the dataset\n",
    "        output_file (str): Path to save the JSONL file\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write each JSON object on a new line\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in input_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def prepare_dataset_files(filename):\n",
    "    \"\"\"\n",
    "    Prepare the dataset files in the required structure.\n",
    "    \"\"\"\n",
    "    # Sample data (replace with your actual data)\n",
    "    data = load_json(filename)\n",
    "    \n",
    "    # Create the data directory\n",
    "    data_dir = Path('datasets', TICKER, MASK_NAME+\"-finetune-dataset\")\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Convert and save the training data\n",
    "    convert_to_jsonl(data, str(data_dir / 'train.jsonl'))\n",
    "    \n",
    "    # Create the dataset_dict configuration\n",
    "    dataset_dict = {\n",
    "        \"train\": str(data_dir / 'train.jsonl')\n",
    "    }\n",
    "    \n",
    "    # Save the dataset configuration\n",
    "    with open(data_dir / 'dataset_dict.json', 'w') as f:\n",
    "        json.dump(dataset_dict, f, indent=2)\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = os.path.join(\"datasets\", TICKER, TICKER+\".json\")\n",
    "    prepare_dataset_files(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
